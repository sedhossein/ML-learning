### ANI

#### intro
- Can learn everything/function
- Resistant against noises
- We can decide on connections
- Adaptation power with area changes
- Can work with minimal data
- Can learn on noisily data
- Its inspiration based on nature 


#### concepts
- Neurons
- Activation functions
- Feed forward network
- Recurrent network


### type of ANI Architecture
- Perceptron
    - good to linear separation
    - two learning model
    1. perceptron rule
        - change weights randomly
        - consider learning rate
    2. delta rule
        - gradient descent 
        - Back propagation(BP) Algorithm
            - compute sigmoid function
    
            
- Multi-layer perceptron(MLP)
    - more hidden internal layers, more nodes, more connection, need more time to learn, more data to learn, more power
    - over fitting is near u!
        - use validation set  based on training set
        - weight decay
        - k-fold cross validation
- Simulated annealing
- Hybrid global learning
- genetic algorithms
- recurrent network
- Radial basis function(RBF)
    - single hidden layer(radial unit)
    - gaussian 
- Hopfield 
    - face detection
    - Hebb
- Kohonen(SOM-self organizing maps)

- ...

* we need pre-process before using these data + normalization

